import 'dart:async';
import 'dart:convert';

import 'package:dio/dio.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter_dotenv/flutter_dotenv.dart';

import '../../../../AI/core/ai_constants.dart';
import '../../../../AI/core/ai_logger.dart';
import '../services/conversation_window_manager.dart';
import '../services/token_counter.dart';

/// Gemini API ì‘ë‹µ (í† í° ì‚¬ìš©ëŸ‰ í¬í•¨)
class GeminiResponse {
  final String content;
  final int? promptTokenCount;
  final int? candidatesTokenCount;
  final int? totalTokenCount;
  final int? thoughtsTokenCount;
  /// ì‘ë‹µì´ MAX_TOKENSë¡œ ì˜ë ¸ëŠ”ì§€ ì—¬ë¶€
  final bool wasTruncated;
  /// finishReason (STOP, MAX_TOKENS, SAFETY ë“±)
  final String? finishReason;

  const GeminiResponse({
    required this.content,
    this.promptTokenCount,
    this.candidatesTokenCount,
    this.totalTokenCount,
    this.thoughtsTokenCount,
    this.wasTruncated = false,
    this.finishReason,
  });

  /// ì´ í† í° ì‚¬ìš©ëŸ‰ (AI ì‘ë‹µ ì €ì¥ìš©)
  int? get tokensUsed => totalTokenCount;
}

/// Gemini 3.0 REST API ë°ì´í„°ì†ŒìŠ¤
///
/// REST APIë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ Gemini 3.0 ì‚¬ìš©
/// í† í° ì œí•œ ê´€ë¦¬ í¬í•¨ (ConversationWindowManager)
class GeminiRestDatasource {
  late final Dio _dio;
  final List<Map<String, dynamic>> _conversationHistory = [];
  String? _systemPrompt;
  bool _isInitialized = false;

  /// ëŒ€í™” ìœˆë„ìš° ê´€ë¦¬ì (í† í° ì œí•œ)
  final ConversationWindowManager _windowManager = ConversationWindowManager();

  /// ë§ˆì§€ë§‰ íŠ¸ë¦¬ë° ì •ë³´
  WindowedConversation? _lastWindowResult;

  static const String _baseUrl =
      'https://generativelanguage.googleapis.com/v1beta';
  static const String _model = 'gemini-3-flash-preview'; // Gemini 3.0 Flash (ë¹ ë¥¸ ì‘ë‹µ)

  /// í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
  static String get _apiKey => dotenv.env['GEMINI_API_KEY'] ?? '';

  /// ì´ˆê¸°í™” ìƒíƒœ
  bool get isInitialized => _isInitialized;

  /// ì´ˆê¸°í™”
  void initialize({String? apiKey}) {
    final key = apiKey ?? _apiKey;
    if (key.isEmpty || key == 'YOUR_API_KEY_HERE') {
      _isInitialized = false;
      return;
    }

    _dio = Dio(BaseOptions(
      baseUrl: _baseUrl,
      headers: {
        'Content-Type': 'application/json',
      },
      queryParameters: {
        'key': key,
      },
    ));

    _isInitialized = true;
  }

  /// ìƒˆ ì±„íŒ… ì„¸ì…˜ ì‹œì‘
  void startNewSession(String systemPrompt) {
    _conversationHistory.clear();
    _systemPrompt = systemPrompt;
    _windowManager.setSystemPrompt(systemPrompt);
    _lastWindowResult = null;

    if (kDebugMode) {
      final promptTokens = TokenCounter.estimateSystemPromptTokens(systemPrompt);
      print('[GeminiDatasource] ìƒˆ ì„¸ì…˜ ì‹œì‘, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í°: $promptTokens');
    }
  }

  /// ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ë°›ê¸° (í† í° ì‚¬ìš©ëŸ‰ í¬í•¨)
  Future<GeminiResponse> sendMessageWithMetadata(String message) async {
    if (!_isInitialized) {
      return GeminiResponse(content: _getMockResponse(message));
    }

    try {
      // ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
      _conversationHistory.add({
        'role': 'user',
        'parts': [
          {'text': message}
        ],
      });

      final response = await _dio.post(
        '/models/$_model:generateContent',
        data: _buildRequestBody(),
      );

      final responseData = response.data as Map<String, dynamic>;
      final candidates = responseData['candidates'] as List?;

      if (candidates == null || candidates.isEmpty) {
        return const GeminiResponse(content: 'ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.');
      }

      final content = candidates[0]['content'] as Map<String, dynamic>;
      final parts = content['parts'] as List;
      final text = parts[0]['text'] as String;

      // usageMetadata íŒŒì‹±
      final usageMetadata = responseData['usageMetadata'] as Map<String, dynamic>?;
      final promptTokenCount = usageMetadata?['promptTokenCount'] as int?;
      final candidatesTokenCount = usageMetadata?['candidatesTokenCount'] as int?;
      final totalTokenCount = usageMetadata?['totalTokenCount'] as int?;
      final thoughtsTokenCount = usageMetadata?['thoughtsTokenCount'] as int?;

      // ëŒ€í™” ê¸°ë¡ì— AI ì‘ë‹µ ì¶”ê°€
      _conversationHistory.add({
        'role': 'model',
        'parts': [
          {'text': text}
        ],
      });

      // ë¡œì»¬ ë¡œê·¸ ì €ì¥
      await AiLogger.log(
        provider: 'gemini',
        model: _model,
        type: 'chat',
        request: {
          'message': message,
          'system_prompt_length': _systemPrompt?.length ?? 0,
        },
        response: {'content': text},
        tokens: {
          'prompt': promptTokenCount,
          'completion': candidatesTokenCount,
          'total': totalTokenCount,
          'thoughts': thoughtsTokenCount,
        },
        costUsd: _calculateCost(promptTokenCount ?? 0, candidatesTokenCount ?? 0),
        success: true,
      );

      return GeminiResponse(
        content: text,
        promptTokenCount: promptTokenCount,
        candidatesTokenCount: candidatesTokenCount,
        totalTokenCount: totalTokenCount,
        thoughtsTokenCount: thoughtsTokenCount,
      );
    } on DioException catch (e) {
      if (e.response != null) {
        final errorData = e.response?.data;
        throw Exception('API ì˜¤ë¥˜: ${errorData?['error']?['message'] ?? e.message}');
      }
      throw Exception('ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ${e.message}');
    } catch (e) {
      throw Exception('AI ì‘ë‹µ ì˜¤ë¥˜: $e');
    }
  }

  /// ë©”ì‹œì§€ ì „ì†¡ ë° ì‘ë‹µ ë°›ê¸° (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
  Future<String> sendMessage(String message) async {
    final response = await sendMessageWithMetadata(message);
    return response.content;
  }

  /// ë§ˆì§€ë§‰ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì˜ í† í° ì‚¬ìš©ëŸ‰ (ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì¡°íšŒ ê°€ëŠ¥)
  GeminiResponse? _lastStreamingResponse;

  /// ë§ˆì§€ë§‰ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì˜ í† í° ì‚¬ìš©ëŸ‰ getter
  GeminiResponse? get lastStreamingResponse => _lastStreamingResponse;

  /// ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (í† í° ì‚¬ìš©ëŸ‰ì€ lastStreamingResponseì—ì„œ ì¡°íšŒ)
  Stream<String> sendMessageStream(String message) async* {
    _lastStreamingResponse = null;

    if (!_isInitialized) {
      final mockResponse = _getMockResponse(message);
      for (int i = 0; i < mockResponse.length; i++) {
        await Future.delayed(const Duration(milliseconds: 20));
        yield mockResponse.substring(0, i + 1);
      }
      _lastStreamingResponse = GeminiResponse(content: mockResponse);
      return;
    }

    try {
      // ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
      _conversationHistory.add({
        'role': 'user',
        'parts': [
          {'text': message}
        ],
      });

      final response = await _dio.post<ResponseBody>(
        '/models/$_model:streamGenerateContent',
        data: _buildRequestBody(),
        queryParameters: {'alt': 'sse'},
        options: Options(responseType: ResponseType.stream),
      );

      String accumulated = '';
      final stream = response.data!.stream;

      // í† í° ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘ (ë§ˆì§€ë§‰ ì²­í¬ì—ì„œ)
      int? promptTokenCount;
      int? candidatesTokenCount;
      int? totalTokenCount;
      int? thoughtsTokenCount;

      // ğŸ”§ finishReason ì¶”ì  (MAX_TOKENS ê°ì§€ìš©)
      String? lastFinishReason;
      bool wasTruncated = false;

      await for (final chunk in stream) {
        final chunkStr = utf8.decode(chunk);
        final lines = chunkStr.split('\n');

        for (final line in lines) {
          if (line.startsWith('data: ')) {
            final jsonStr = line.substring(6);
            if (jsonStr.trim().isEmpty) continue;

            try {
              final data = jsonDecode(jsonStr) as Map<String, dynamic>;

              // usageMetadata íŒŒì‹± (ë§ˆì§€ë§‰ ì²­í¬ì— í¬í•¨ë¨)
              final usageMetadata = data['usageMetadata'] as Map<String, dynamic>?;
              if (usageMetadata != null) {
                promptTokenCount = usageMetadata['promptTokenCount'] as int?;
                candidatesTokenCount = usageMetadata['candidatesTokenCount'] as int?;
                totalTokenCount = usageMetadata['totalTokenCount'] as int?;
                thoughtsTokenCount = usageMetadata['thoughtsTokenCount'] as int?;
              }

              final candidates = data['candidates'] as List?;
              if (candidates != null && candidates.isNotEmpty) {
                // ğŸ”§ finishReason í™•ì¸ (MAX_TOKENSë©´ ì‘ë‹µì´ ì˜ë¦° ê²ƒ)
                final finishReason = candidates[0]['finishReason'] as String?;
                if (finishReason != null) {
                  lastFinishReason = finishReason;
                  if (finishReason == 'MAX_TOKENS') {
                    wasTruncated = true;
                    if (kDebugMode) {
                      print('[GeminiDatasource] âš ï¸ ì‘ë‹µì´ MAX_TOKENSë¡œ ì˜ë¦¼! candidatesTokenCount: $candidatesTokenCount');
                    }
                  }
                }

                final content = candidates[0]['content'] as Map<String, dynamic>?;
                if (content != null) {
                  final parts = content['parts'] as List?;
                  if (parts != null && parts.isNotEmpty) {
                    final text = parts[0]['text'] as String?;
                    if (text != null) {
                      accumulated += text;
                      yield accumulated;
                    }
                  }
                }
              }
            } catch (e) {
              // ğŸ”§ íŒŒì‹± ì˜¤ë¥˜ ë¡œê¹… (ë””ë²„ê·¸ ëª¨ë“œ)
              if (kDebugMode) {
                print('[GeminiDatasource] JSON íŒŒì‹± ì˜¤ë¥˜: $e');
              }
            }
          }
        }
      }

      // ğŸ”§ ì‘ë‹µ ì˜ë¦¼ ê°ì§€ ì‹œ ë¡œê·¸
      if (kDebugMode && lastFinishReason != null) {
        print('[GeminiDatasource] ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ - finishReason: $lastFinishReason, wasTruncated: $wasTruncated');
      }

      // ëŒ€í™” ê¸°ë¡ì— AI ì‘ë‹µ ì¶”ê°€
      if (accumulated.isNotEmpty) {
        _conversationHistory.add({
          'role': 'model',
          'parts': [
            {'text': accumulated}
          ],
        });
      }

      // ë§ˆì§€ë§‰ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì €ì¥ (í† í° ì‚¬ìš©ëŸ‰ + ì˜ë¦¼ ì—¬ë¶€ í¬í•¨)
      _lastStreamingResponse = GeminiResponse(
        content: accumulated,
        promptTokenCount: promptTokenCount,
        candidatesTokenCount: candidatesTokenCount,
        totalTokenCount: totalTokenCount,
        thoughtsTokenCount: thoughtsTokenCount,
        wasTruncated: wasTruncated,
        finishReason: lastFinishReason,
      );

      // ë¡œì»¬ ë¡œê·¸ ì €ì¥
      await AiLogger.log(
        provider: 'gemini',
        model: _model,
        type: 'chat_stream',
        request: {
          'message': message,
          'system_prompt_length': _systemPrompt?.length ?? 0,
        },
        response: {'content': accumulated},
        tokens: {
          'prompt': promptTokenCount,
          'completion': candidatesTokenCount,
          'total': totalTokenCount,
          'thoughts': thoughtsTokenCount,
        },
        costUsd: _calculateCost(promptTokenCount ?? 0, candidatesTokenCount ?? 0),
        success: true,
      );
    } on DioException catch (e) {
      throw Exception('ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: ${e.message}');
    } catch (e) {
      throw Exception('AI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: $e');
    }
  }

  /// ìš”ì²­ ë³¸ë¬¸ ìƒì„± (í† í° ì œí•œ ì ìš©)
  Map<String, dynamic> _buildRequestBody() {
    // í† í° ì œí•œì— ë§ê²Œ ëŒ€í™” ìœˆë„ìš°ì‰
    _lastWindowResult = _windowManager.windowMessages(_conversationHistory);

    if (kDebugMode && _lastWindowResult!.wasTrimmed) {
      print('[GeminiDatasource] í† í° ì œí•œìœ¼ë¡œ ${_lastWindowResult!.removedCount}ê°œ ë©”ì‹œì§€ íŠ¸ë¦¬ë°');
      print('[GeminiDatasource] í˜„ì¬ í† í°: ${_lastWindowResult!.estimatedTokens}');
    }

    final windowedMessages = _lastWindowResult!.messages;
    final contents = <Map<String, dynamic>>[];

    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ì— í¬í•¨
    if (_systemPrompt != null && windowedMessages.isNotEmpty) {
      final firstUserMsg = windowedMessages.first;
      if (firstUserMsg['role'] == 'user') {
        contents.add({
          'role': 'user',
          'parts': [
            {'text': '$_systemPrompt\n\n${firstUserMsg['parts'][0]['text']}'}
          ],
        });
        contents.addAll(windowedMessages.skip(1));
      } else {
        contents.addAll(windowedMessages);
      }
    } else {
      contents.addAll(windowedMessages);
    }

    // [ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€]
    final maxTokens = TokenLimits.questionAnswerMaxTokens;
    if (kDebugMode) {
      print('[gemini_rest_datasource.dart] _buildRequestBody: maxOutputTokens = $maxTokens');
    }

    return {
      'contents': contents,
      'generationConfig': {
        'temperature': 0.7,
        'topK': 40,
        'topP': 0.95,
        'maxOutputTokens': maxTokens,
      },
      'safetySettings': [
        {
          'category': 'HARM_CATEGORY_HARASSMENT',
          'threshold': 'BLOCK_MEDIUM_AND_ABOVE'
        },
        {
          'category': 'HARM_CATEGORY_HATE_SPEECH',
          'threshold': 'BLOCK_MEDIUM_AND_ABOVE'
        },
        {
          'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
          'threshold': 'BLOCK_MEDIUM_AND_ABOVE'
        },
        {
          'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',
          'threshold': 'BLOCK_MEDIUM_AND_ABOVE'
        },
      ],
    };
  }

  /// í˜„ì¬ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ì¡°íšŒ
  TokenUsageInfo getTokenUsageInfo() {
    return _windowManager.getTokenUsageInfo(_conversationHistory);
  }

  /// ë§ˆì§€ë§‰ ìœˆë„ìš°ì‰ ê²°ê³¼ ì¡°íšŒ
  WindowedConversation? get lastWindowResult => _lastWindowResult;

  /// Mock ì‘ë‹µ ìƒì„±
  String _getMockResponse(String userMessage) {
    final lowercaseMessage = userMessage.toLowerCase();

    if (lowercaseMessage.contains('ì˜¤ëŠ˜') || lowercaseMessage.contains('ìš´ì„¸')) {
      return '''ì˜¤ëŠ˜ì˜ ìš´ì„¸ë¥¼ ë´ë“œë¦´ê²Œìš”.

ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ê¸°ìš´ì´ ê°ë„ëŠ” í•˜ë£¨ì…ë‹ˆë‹¤. íŠ¹íˆ ëŒ€ì¸ê´€ê³„ì—ì„œ ê¸ì •ì ì¸ ì¼ì´ ìƒê¸¸ ìˆ˜ ìˆì–´ìš”.

ì˜¤ì „ì—ëŠ” ì¡°ê¸ˆ í”¼ê³¤í•  ìˆ˜ ìˆìœ¼ë‹ˆ ë¬´ë¦¬í•˜ì§€ ë§ˆì‹œê³ , ì˜¤í›„ë¶€í„° í™œë ¥ì´ ìƒê¸°ì‹¤ ê±°ì˜ˆìš”.

ì¬ë¬¼ìš´: â˜…â˜…â˜…â˜…â˜†
ì• ì •ìš´: â˜…â˜…â˜…â˜…â˜…
ê±´ê°•ìš´: â˜…â˜…â˜…â˜†â˜†

ì˜¤ëŠ˜ í•˜ë£¨ë„ ì¢‹ì€ ì¼ë§Œ ê°€ë“í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!''';
    }

    if (lowercaseMessage.contains('ì‚¬ì£¼') || lowercaseMessage.contains('ìƒë…„ì›”ì¼')) {
      return '''ì‚¬ì£¼ ë¶„ì„ì„ ë„ì™€ë“œë¦´ê²Œìš”.

ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ìƒë…„ì›”ì¼ê³¼ íƒœì–´ë‚œ ì‹œê°„ì„ ì•Œë ¤ì£¼ì„¸ìš”.

ì˜ˆì‹œ: 1990ë…„ 1ì›” 15ì¼ ì˜¤ì „ 10ì‹œ

ì‹œê°„ê¹Œì§€ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ì‚¬ì£¼íŒ”ìë¥¼ ë¶„ì„í•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.''';
    }

    if (lowercaseMessage.contains('ê¶í•©')) {
      return '''ê¶í•©ì„ ë´ë“œë¦´ê²Œìš”.

ë³¸ì¸ê³¼ ìƒëŒ€ë°©ì˜ ìƒë…„ì›”ì¼ì„ ì•Œë ¤ì£¼ì‹œë©´ ë‘ ë¶„ì˜ ê¶í•©ì„ ë¶„ì„í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ì˜ˆì‹œ:
- ë³¸ì¸: 1990ë…„ 1ì›” 15ì¼
- ìƒëŒ€ë°©: 1992ë…„ 3ì›” 20ì¼

ë‚ ì§œë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ìƒì„¸í•œ ê¶í•© ë¶„ì„ì„ í•´ë“œë¦´ê²Œìš”!''';
    }

    return '''ë„¤, ë§ì”€í•´ ì£¼ì„¸ìš”.

ì €ëŠ” ì‚¬ì£¼, ìš´ì„¸, ê¶í•© ë“±ì— ëŒ€í•´ ìƒë‹´í•´ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.

ì–´ë–¤ ê²ƒì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?
- ì˜¤ëŠ˜ì˜ ìš´ì„¸
- ì‚¬ì£¼ ë¶„ì„
- ê¶í•© ë³´ê¸°
- ê¸°íƒ€ ì§ˆë¬¸''';
  }

  /// ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  void dispose() {
    _conversationHistory.clear();
    _systemPrompt = null;
  }

  /// Gemini ë¹„ìš© ê³„ì‚° (USD)
  /// gemini-3-flash: ì…ë ¥ $0.50/1M, ì¶œë ¥ $3.00/1M
  double _calculateCost(int promptTokens, int completionTokens) {
    const inputPrice = 0.50 / 1000000;
    const outputPrice = 3.00 / 1000000;
    return (promptTokens * inputPrice) + (completionTokens * outputPrice);
  }
}
