/// # AI API ì„œë¹„ìŠ¤
///
/// ## ê°œìš”
/// OpenAI(GPT-5.2) ë° Google Gemini API í˜¸ì¶œì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
/// **ë³´ì•ˆ**: API í‚¤ëŠ” í´ë¼ì´ì–¸íŠ¸ì— ë…¸ì¶œí•˜ì§€ ì•Šê³ , Supabase Edge Functionì„ í†µí•´ í˜¸ì¶œí•©ë‹ˆë‹¤.
///
/// ## íŒŒì¼ ìœ„ì¹˜
/// `frontend/lib/AI/services/ai_api_service.dart`
///
/// ## ì•„í‚¤í…ì²˜
/// ```
/// Flutter App
///     â†“
/// AiApiService.callOpenAI() / callGemini()
///     â†“
/// Supabase Edge Function (ai-openai, ai-gemini)
///     â†“
/// OpenAI API / Google Gemini API
///     â†“
/// ì‘ë‹µ (JSON)
/// ```
///
/// ## Edge Function ì—­í• 
/// - API í‚¤ ë³´ì•ˆ (Supabase í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬)
/// - CORS ì²˜ë¦¬
/// - ì‘ë‹µ í‘œì¤€í™” (ì„±ê³µ/ì‹¤íŒ¨, í† í° ì‚¬ìš©ëŸ‰)
///
/// ## ì‚¬ìš© ì˜ˆì‹œ
/// ```dart
/// final service = AiApiService();
///
/// // GPT-5.2 í˜¸ì¶œ (í‰ìƒ ì‚¬ì£¼)
/// final gptResponse = await service.callOpenAI(
///   messages: prompt.buildMessages(inputData),
///   model: 'gpt-5.2',
///   maxTokens: 4096,
///   temperature: 0.7,
/// );
///
/// // Gemini í˜¸ì¶œ (ì¼ìš´)
/// final geminiResponse = await service.callGemini(
///   messages: prompt.buildMessages(inputData),
///   model: 'gemini-2.0-flash',
///   maxTokens: 2048,
///   temperature: 0.8,
/// );
/// ```
///
/// ## ì‘ë‹µ í˜•ì‹ (AiApiResponse)
/// ```dart
/// AiApiResponse(
///   success: true,
///   content: {'summary': '...', 'personality': {...}},
///   promptTokens: 1200,
///   completionTokens: 800,
///   cachedTokens: 0,
///   totalCostUsd: 0.032,
/// )
/// ```
///
/// ## ë¹„ìš© ê³„ì‚°
/// - OpenAI: `OpenAIPricing.calculateCost()` ì‚¬ìš©
/// - Gemini: `GeminiPricing.calculateCost()` ì‚¬ìš©
/// - ê²°ê³¼ëŠ” `ai_summaries.total_cost_usd`ì— ì €ì¥
///
/// ## ê´€ë ¨ íŒŒì¼
/// - `supabase/functions/ai-openai/index.ts`: OpenAI Edge Function
/// - `supabase/functions/ai-gemini/index.ts`: Gemini Edge Function
/// - `ai_constants.dart`: ëª¨ë¸ëª…, ê°€ê²© ì •ë³´

import 'dart:async';
import 'dart:convert';

import 'package:supabase_flutter/supabase_flutter.dart';

import '../core/ai_constants.dart';
import '../core/ai_logger.dart';

/// API ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤
///
/// ## í•„ë“œ ì„¤ëª…
/// | í•„ë“œ | íƒ€ì… | ì„¤ëª… |
/// |------|------|------|
/// | success | bool | í˜¸ì¶œ ì„±ê³µ ì—¬ë¶€ |
/// | content | Map? | AI ì‘ë‹µ (íŒŒì‹±ëœ JSON) |
/// | error | String? | ì˜¤ë¥˜ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ) |
/// | promptTokens | int? | ì…ë ¥ í† í° ìˆ˜ |
/// | completionTokens | int? | ì¶œë ¥ í† í° ìˆ˜ |
/// | cachedTokens | int? | ìºì‹œëœ í† í° ìˆ˜ (OpenAIë§Œ) |
/// | totalCostUsd | double? | ê³„ì‚°ëœ ë¹„ìš© (USD) |
class AiApiResponse {
  /// í˜¸ì¶œ ì„±ê³µ ì—¬ë¶€
  final bool success;

  /// AI ì‘ë‹µ (íŒŒì‹±ëœ JSON)
  /// í”„ë¡¬í”„íŠ¸ì—ì„œ ì§€ì •í•œ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” êµ¬ì¡°
  final Map<String, dynamic>? content;

  /// ì˜¤ë¥˜ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
  final String? error;

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš©
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  /// ì…ë ¥ í† í° ìˆ˜ (í”„ë¡¬í”„íŠ¸)
  final int? promptTokens;

  /// ì¶œë ¥ í† í° ìˆ˜ (ì‘ë‹µ)
  final int? completionTokens;

  /// ìºì‹œëœ í† í° ìˆ˜ (OpenAI Prompt Caching)
  /// 50% í• ì¸ëœ ê°€ê²©ìœ¼ë¡œ ê³„ì‚°ë¨
  final int? cachedTokens;

  /// ì´ ë¹„ìš© (USD)
  /// OpenAIPricing/GeminiPricingìœ¼ë¡œ ê³„ì‚°
  final double? totalCostUsd;

  const AiApiResponse({
    required this.success,
    this.content,
    this.error,
    this.promptTokens,
    this.completionTokens,
    this.cachedTokens,
    this.totalCostUsd,
  });

  factory AiApiResponse.success({
    required Map<String, dynamic> content,
    int? promptTokens,
    int? completionTokens,
    int? cachedTokens,
    double? totalCostUsd,
  }) =>
      AiApiResponse(
        success: true,
        content: content,
        promptTokens: promptTokens,
        completionTokens: completionTokens,
        cachedTokens: cachedTokens,
        totalCostUsd: totalCostUsd,
      );

  factory AiApiResponse.failure(String error) => AiApiResponse(
        success: false,
        error: error,
      );
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// AI API ì„œë¹„ìŠ¤
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/// AI API ì„œë¹„ìŠ¤ (Edge Function í˜¸ì¶œ)
///
/// ## ì‹±ê¸€í†¤ ì‚¬ìš© ì˜ˆì‹œ
/// ```dart
/// final service = AiApiService();
/// // ë˜ëŠ” saju_analysis_service.dartì—ì„œ ì£¼ì…
/// ```
class AiApiService {
  /// Supabase í´ë¼ì´ì–¸íŠ¸ (ì‹±ê¸€í†¤)
  SupabaseClient get _client => Supabase.instance.client;

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // v24 Background ëª¨ë“œ Polling ì„¤ì •
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  /// ìµœëŒ€ í´ë§ íšŸìˆ˜ (GPT-5.2 reasoning: 60-120ì´ˆ, ìµœëŒ€ 4ë¶„)
  static const int _maxPollingAttempts = 120;

  /// í´ë§ ê°„ê²© (2ì´ˆ Ã— 120íšŒ = ìµœëŒ€ 240ì´ˆ)
  static const Duration _pollingInterval = Duration(seconds: 2);

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // OpenAI API (GPT-5.2)
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  /// OpenAI API í˜¸ì¶œ (GPT-5.2 - v24 Background ëª¨ë“œ ì§€ì›)
  ///
  /// ## Edge Function
  /// `supabase/functions/ai-openai/index.ts` (v24)
  ///
  /// ## v24 Background ëª¨ë“œ (ê¸°ë³¸ê°’)
  /// - Supabase 150ì´ˆ walltime ì œí•œ ì™„ì „ íšŒí”¼
  /// - OpenAI Responses API background=true ëª¨ë“œ ì‚¬ìš©
  /// - task_id ë°˜í™˜ â†’ ai-openai-resultë¡œ polling
  ///
  /// ## íŒŒë¼ë¯¸í„°
  /// - `messages`: [{role: 'system', content: ...}, {role: 'user', content: ...}]
  /// - `model`: ëª¨ë¸ ID (ê¸°ë³¸: 'gpt-5.2')
  /// - `maxTokens`: ìµœëŒ€ ì‘ë‹µ í† í° (ê¸°ë³¸: 2000)
  /// - `temperature`: ì°½ì˜ì„± (0.0~2.0, ê¸°ë³¸: 0.7)
  /// - `logType`: ë¡œê·¸ ë¶„ë¥˜ (ê¸°ë³¸: 'unknown')
  /// - `runInBackground`: Background ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸: true)
  ///
  /// ## ì‘ë‹µ ì²˜ë¦¬
  /// 1. Edge Function í˜¸ì¶œ (run_in_background: true)
  /// 2. task_id ë°˜í™˜ ë°›ìŒ
  /// 3. ai-openai-result Edge Functionìœ¼ë¡œ polling
  /// 4. completed ìƒíƒœì¼ ë•Œ ê²°ê³¼ ë°˜í™˜
  /// 5. í† í° ì‚¬ìš©ëŸ‰/ë¹„ìš© ê³„ì‚°
  /// 6. ë¡œì»¬ ë¡œê·¸ ì €ì¥
  /// v29: taskType íŒŒë¼ë¯¸í„° ì¶”ê°€ - ë³‘ë ¬ ì‹¤í–‰ ì‹œ task ë¶„ë¦¬ìš©
  /// - saju_base: í‰ìƒìš´ì„¸
  /// - monthly_fortune: ì›”ë³„ìš´ì„¸
  /// - yearly_2026: 2026 ì‹ ë…„ìš´ì„¸
  /// - yearly_2025: 2025 íšŒê³ ìš´ì„¸
  /// - daily_fortune: ì˜¤ëŠ˜ì˜ ì¼ìš´
  Future<AiApiResponse> callOpenAI({
    required List<Map<String, String>> messages,
    required String model,
    int maxTokens = 2000,
    double temperature = 0.7,
    String logType = 'unknown',
    String? userId,
    bool runInBackground = true,  // v24: ê¸°ë³¸ê°’ true
    String taskType = 'saju_analysis',  // v29: task êµ¬ë¶„ìš© (ê¸°ë³¸ê°’ ìœ ì§€)
  }) async {
    try {
      print('[AiApiService v29] OpenAI í˜¸ì¶œ: $model (background=$runInBackground, taskType=$taskType, userId: ${userId ?? "null"})');

      final response = await _client.functions.invoke(
        'ai-openai',
        body: {
          'messages': messages,
          'model': model,
          'max_tokens': maxTokens,
          'temperature': temperature,
          'response_format': {'type': 'json_object'},
          'run_in_background': runInBackground,  // v24: Background ëª¨ë“œ
          'task_type': taskType,  // v29: ë³‘ë ¬ ì‹¤í–‰ ì‹œ task ë¶„ë¦¬!
          if (userId != null) 'user_id': userId,
        },
      );

      if (response.status != 200) {
        final error = response.data?['error'] ?? 'OpenAI API ì˜¤ë¥˜';
        print('[AiApiService v24] OpenAI ì˜¤ë¥˜: $error');
        return AiApiResponse.failure(error.toString());
      }

      final data = response.data as Map<String, dynamic>;

      // v24: Background ëª¨ë“œì¸ ê²½ìš° task_idë¡œ polling
      if (runInBackground && data['task_id'] != null) {
        final taskId = data['task_id'] as String;
        final openaiResponseId = data['openai_response_id'] as String?;
        print('[AiApiService v24] Task created: $taskId');
        print('[AiApiService v24] OpenAI Response ID: $openaiResponseId');

        // Pollingìœ¼ë¡œ ê²°ê³¼ ëŒ€ê¸°
        return await _pollForOpenAIResult(
          taskId: taskId,
          model: model,
          logType: logType,
          messages: messages,
          maxTokens: maxTokens,
          temperature: temperature,
        );
      }

      // Sync ëª¨ë“œ (runInBackground=false) ë˜ëŠ” ë ˆê±°ì‹œ ì‘ë‹µ
      final content = _parseJsonContent(data['content'] as String?);

      // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
      final usage = data['usage'] as Map<String, dynamic>?;
      final promptTokens = usage?['prompt_tokens'] as int?;
      final completionTokens = usage?['completion_tokens'] as int?;
      final cachedTokens = usage?['cached_tokens'] as int? ?? 0;

      // ë¹„ìš© ê³„ì‚°
      final totalCostUsd = _calculateOpenAICost(
        model: model,
        promptTokens: promptTokens ?? 0,
        completionTokens: completionTokens ?? 0,
        cachedTokens: cachedTokens,
      );

      print('[AiApiService v24] OpenAI ì™„ë£Œ (sync): prompt=$promptTokens, completion=$completionTokens');

      // ë¡œì»¬ ë¡œê·¸ ì €ì¥
      await AiLogger.log(
        provider: 'openai',
        model: model,
        type: logType,
        request: {
          'messages': messages,
          'max_tokens': maxTokens,
          'temperature': temperature,
        },
        response: content,
        tokens: {
          'prompt': promptTokens,
          'completion': completionTokens,
          'cached': cachedTokens,
        },
        costUsd: totalCostUsd,
        success: true,
      );

      return AiApiResponse.success(
        content: content,
        promptTokens: promptTokens,
        completionTokens: completionTokens,
        cachedTokens: cachedTokens,
        totalCostUsd: totalCostUsd,
      );
    } catch (e) {
      print('[AiApiService v24] OpenAI ì˜ˆì™¸: $e');

      // ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
      await AiLogger.log(
        provider: 'openai',
        model: model,
        type: logType,
        request: {
          'messages': messages,
          'max_tokens': maxTokens,
          'temperature': temperature,
        },
        success: false,
        error: e.toString(),
      );

      return AiApiResponse.failure(e.toString());
    }
  }

  /// v24: Task ê²°ê³¼ Polling (OpenAI Responses API)
  ///
  /// ai-openai-result Edge Function í˜¸ì¶œ
  /// â†’ OpenAI /v1/responses/{id} ì§ì ‘ polling
  /// â†’ ìƒíƒœ: queued â†’ in_progress â†’ completed
  Future<AiApiResponse> _pollForOpenAIResult({
    required String taskId,
    required String model,
    required String logType,
    required List<Map<String, String>> messages,
    required int maxTokens,
    required double temperature,
  }) async {
    for (int attempt = 0; attempt < _maxPollingAttempts; attempt++) {
      try {
        final response = await _client.functions.invoke(
          'ai-openai-result',
          body: {'task_id': taskId},
        );

        if (response.status != 200) {
          print('[AiApiService v24] Polling error: status=${response.status}');
          await Future.delayed(_pollingInterval);
          continue;
        }

        final data = response.data as Map<String, dynamic>;
        final status = data['status'] as String?;

        if (attempt % 5 == 0) {
          print('[AiApiService v24] Polling attempt $attempt: status=$status');
        }

        switch (status) {
          case 'completed':
            print('[AiApiService v24] Task completed after $attempt attempts');

            // v24: contentê°€ ìµœìƒìœ„ ë ˆë²¨ì— ìˆìŒ
            final contentStr = data['content'] as String?;
            final content = _parseJsonContent(contentStr);

            // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            final usage = data['usage'] as Map<String, dynamic>?;
            final promptTokens = usage?['prompt_tokens'] as int?;
            final completionTokens = usage?['completion_tokens'] as int?;
            final cachedTokens = usage?['cached_tokens'] as int? ?? 0;

            // ë¹„ìš© ê³„ì‚°
            final totalCostUsd = _calculateOpenAICost(
              model: model,
              promptTokens: promptTokens ?? 0,
              completionTokens: completionTokens ?? 0,
              cachedTokens: cachedTokens,
            );

            print('[AiApiService v24] OpenAI ì™„ë£Œ (polling): prompt=$promptTokens, completion=$completionTokens');

            // ë¡œì»¬ ë¡œê·¸ ì €ì¥
            await AiLogger.log(
              provider: 'openai',
              model: model,
              type: logType,
              request: {
                'messages': messages,
                'max_tokens': maxTokens,
                'temperature': temperature,
                'task_id': taskId,
              },
              response: content,
              tokens: {
                'prompt': promptTokens,
                'completion': completionTokens,
                'cached': cachedTokens,
              },
              costUsd: totalCostUsd,
              success: true,
            );

            return AiApiResponse.success(
              content: content,
              promptTokens: promptTokens,
              completionTokens: completionTokens,
              cachedTokens: cachedTokens,
              totalCostUsd: totalCostUsd,
            );

          case 'incomplete':
            // v25: incomplete ìƒíƒœì—ì„œ contentê°€ ìˆìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            // max_tokens ë„ë‹¬ë¡œ ì‘ë‹µì´ ì˜ë ¸ì§€ë§Œ ë¶€ë¶„ ì½˜í…ì¸  ì‚¬ìš© ê°€ëŠ¥
            final incompleteContent = data['content'] as String?;
            if (incompleteContent != null && incompleteContent.isNotEmpty) {
              print('[AiApiService v25] incomplete but has content (${incompleteContent.length} chars)');

              final parsedContent = _parseJsonContent(incompleteContent);
              final incUsage = data['usage'] as Map<String, dynamic>?;
              final incPromptTokens = incUsage?['prompt_tokens'] as int?;
              final incCompletionTokens = incUsage?['completion_tokens'] as int?;
              final incCachedTokens = incUsage?['cached_tokens'] as int? ?? 0;

              final incTotalCost = _calculateOpenAICost(
                model: model,
                promptTokens: incPromptTokens ?? 0,
                completionTokens: incCompletionTokens ?? 0,
                cachedTokens: incCachedTokens,
              );

              print('[AiApiService v25] incomplete ë¶€ë¶„ ì™„ë£Œ: prompt=$incPromptTokens, completion=$incCompletionTokens');

              await AiLogger.log(
                provider: 'openai',
                model: model,
                type: logType,
                request: {'task_id': taskId, 'finish_reason': 'incomplete'},
                response: parsedContent,
                tokens: {
                  'prompt': incPromptTokens,
                  'completion': incCompletionTokens,
                  'cached': incCachedTokens,
                },
                costUsd: incTotalCost,
                success: true,
              );

              return AiApiResponse.success(
                content: parsedContent,
                promptTokens: incPromptTokens,
                completionTokens: incCompletionTokens,
                cachedTokens: incCachedTokens,
                totalCostUsd: incTotalCost,
              );
            }
            // content ì—†ìœ¼ë©´ ì•„ë˜ failed/cancelledì™€ ê°™ì´ ì‹¤íŒ¨ ì²˜ë¦¬
            final incError = data['error'] ?? 'Task incomplete: no content';
            print('[AiApiService v25] Task incomplete without content');

            await AiLogger.log(
              provider: 'openai',
              model: model,
              type: logType,
              request: {'task_id': taskId},
              success: false,
              error: incError.toString(),
            );

            return AiApiResponse.failure(incError.toString());

          case 'failed':
          case 'cancelled':
            // failed: API ì˜¤ë¥˜
            // cancelled: ì‚¬ìš©ì ë˜ëŠ” ì‹œìŠ¤í…œì— ì˜í•´ ì·¨ì†Œë¨
            final error = data['error'] ?? 'Task $status';
            print('[AiApiService v25] Task $status: $error');

            await AiLogger.log(
              provider: 'openai',
              model: model,
              type: logType,
              request: {'task_id': taskId},
              success: false,
              error: error.toString(),
            );

            return AiApiResponse.failure(error.toString());

          case 'queued':
          case 'in_progress':
            // v24: OpenAI ìƒíƒœê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if (attempt % 10 == 0) {
              print('[AiApiService v24] OpenAI processing... ($status)');
            }
            await Future.delayed(_pollingInterval);
            break;

          case 'pending':
          case 'processing':
            // ë ˆê±°ì‹œ ìƒíƒœê°’ í˜¸í™˜
            await Future.delayed(_pollingInterval);
            break;

          default:
            print('[AiApiService v24] Unknown status: $status');
            await Future.delayed(_pollingInterval);
            break;
        }
      } catch (e) {
        print('[AiApiService v24] Polling error: $e');
        await Future.delayed(_pollingInterval);
      }
    }

    // Timeout
    final error = 'Polling timeout after ${_maxPollingAttempts * 2}s';
    print('[AiApiService v24] $error');

    await AiLogger.log(
      provider: 'openai',
      model: model,
      type: logType,
      request: {'task_id': taskId},
      success: false,
      error: error,
    );

    return AiApiResponse.failure(error);
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // Google Gemini API
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  /// Gemini API í˜¸ì¶œ
  ///
  /// ## Edge Function
  /// `supabase/functions/ai-gemini/index.ts`
  ///
  /// ## íŠ¹ì§•
  /// - GPTë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„ (1-2ì´ˆ)
  /// - ë¹„ìš© ì•½ 25ë°° ì €ë ´
  /// - JSON ì‘ë‹µ ê°•ì œ (`responseMimeType: 'application/json'`)
  ///
  /// ## íŒŒë¼ë¯¸í„°
  /// - `messages`: [{role: 'system', content: ...}, {role: 'user', content: ...}]
  /// - `model`: ëª¨ë¸ ID (ê¸°ë³¸: 'gemini-2.0-flash')
  /// - `maxTokens`: ìµœëŒ€ ì‘ë‹µ í† í° (ê¸°ë³¸: 1000)
  /// - `temperature`: ì°½ì˜ì„± (0.0~2.0, ê¸°ë³¸: 0.8)
  /// - `logType`: ë¡œê·¸ ë¶„ë¥˜ (ê¸°ë³¸: 'unknown')
  Future<AiApiResponse> callGemini({
    required List<Map<String, String>> messages,
    required String model,
    int maxTokens = 1000,
    double temperature = 0.8,
    String logType = 'unknown',
  }) async {
    try {
      print('[AiApiService] Gemini í˜¸ì¶œ: $model');

      final response = await _client.functions.invoke(
        'ai-gemini',
        body: {
          'messages': messages,
          'model': model,
          'max_tokens': maxTokens,
          'temperature': temperature,
        },
      );

      if (response.status != 200) {
        final error = response.data?['error'] ?? 'Gemini API ì˜¤ë¥˜';
        print('[AiApiService] Gemini ì˜¤ë¥˜: $error');
        return AiApiResponse.failure(error.toString());
      }

      final data = response.data as Map<String, dynamic>;
      final content = _parseJsonContent(data['content'] as String?);

      // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
      final usage = data['usage'] as Map<String, dynamic>?;
      final promptTokens = usage?['prompt_tokens'] as int?;
      final completionTokens = usage?['completion_tokens'] as int?;

      // ë¹„ìš© ê³„ì‚°
      final totalCostUsd = _calculateGeminiCost(
        model: model,
        promptTokens: promptTokens ?? 0,
        completionTokens: completionTokens ?? 0,
      );

      print('[AiApiService] Gemini ì™„ë£Œ: prompt=$promptTokens, completion=$completionTokens');

      // ë¡œì»¬ ë¡œê·¸ ì €ì¥
      await AiLogger.log(
        provider: 'gemini',
        model: model,
        type: logType,
        request: {
          'messages': messages,
          'max_tokens': maxTokens,
          'temperature': temperature,
        },
        response: content,
        tokens: {
          'prompt': promptTokens,
          'completion': completionTokens,
        },
        costUsd: totalCostUsd,
        success: true,
      );

      return AiApiResponse.success(
        content: content,
        promptTokens: promptTokens,
        completionTokens: completionTokens,
        totalCostUsd: totalCostUsd,
      );
    } catch (e) {
      print('[AiApiService] Gemini ì˜ˆì™¸: $e');

      // ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
      await AiLogger.log(
        provider: 'gemini',
        model: model,
        type: logType,
        request: {
          'messages': messages,
          'max_tokens': maxTokens,
          'temperature': temperature,
        },
        success: false,
        error: e.toString(),
      );

      return AiApiResponse.failure(e.toString());
    }
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  /// JSON ë¬¸ìì—´ íŒŒì‹± (v26: ê°•í™”ëœ íŒŒì‹± ë¡œì§)
  ///
  /// ## ì²˜ë¦¬ ì¼€ì´ìŠ¤
  /// 1. `{"key": "value"}` â†’ ê·¸ëŒ€ë¡œ íŒŒì‹±
  /// 2. ` ```json\n{...}\n``` ` â†’ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±° í›„ íŒŒì‹±
  /// 3. ì´ì¤‘ ì´ìŠ¤ì¼€ì´í”„ëœ JSON â†’ 2ë‹¨ê³„ íŒŒì‹±
  /// 4. íŒŒì‹± ì‹¤íŒ¨ â†’ `{'raw': content}` ë°˜í™˜ (ìµœí›„ì˜ ìˆ˜ë‹¨)
  ///
  /// ## v26 ê°œì„ 
  /// - ì´ì¤‘ ì´ìŠ¤ì¼€ì´í”„ëœ JSON ì²˜ë¦¬ (\\n, \\", etc.)
  /// - ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ë‚´ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
  /// - ì—¬ëŸ¬ ë²ˆ íŒŒì‹± ì‹œë„ í›„ ì‹¤íŒ¨ ì‹œì—ë§Œ raw ë°˜í™˜
  Map<String, dynamic> _parseJsonContent(String? content) {
    if (content == null || content.isEmpty) {
      return {};
    }

    try {
      // 1ì°¨: ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±°
      String cleaned = content.trim();
      if (cleaned.startsWith('```json')) {
        cleaned = cleaned.substring(7);
      } else if (cleaned.startsWith('```')) {
        cleaned = cleaned.substring(3);
      }
      if (cleaned.endsWith('```')) {
        cleaned = cleaned.substring(0, cleaned.length - 3);
      }
      cleaned = cleaned.trim();

      // 2ì°¨: ì§ì ‘ íŒŒì‹± ì‹œë„
      try {
        return jsonDecode(cleaned) as Map<String, dynamic>;
      } catch (_) {
        // 3ì°¨: ì´ì¤‘ ì´ìŠ¤ì¼€ì´í”„ëœ ê²½ìš° ì²˜ë¦¬
        // AIê°€ JSON ë¬¸ìì—´ì„ í•œ ë²ˆ ë” ì´ìŠ¤ì¼€ì´í”„í•œ ê²½ìš°
        // ì˜ˆ: "{\"summary\": \"...\"}" ëŒ€ì‹  "{\\\"summary\\\": \\\"...\\\"}"
        if (cleaned.contains(r'\"') || cleaned.contains(r'\n')) {
          try {
            // ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì–¸ì´ìŠ¤ì¼€ì´í”„
            final unescaped = cleaned
                .replaceAll(r'\"', '"')
                .replaceAll(r'\n', '\n')
                .replaceAll(r'\t', '\t')
                .replaceAll(r'\\', r'\');
            return jsonDecode(unescaped) as Map<String, dynamic>;
          } catch (_) {
            // 4ì°¨: JSON ë¬¸ìì—´ë¡œ ë˜í•‘ëœ ê²½ìš°
            // ì˜ˆ: "{\\"summary\\": \\"...\\"}"
            if (cleaned.startsWith('"') && cleaned.endsWith('"')) {
              try {
                final inner = jsonDecode(cleaned) as String;
                return jsonDecode(inner) as Map<String, dynamic>;
              } catch (_) {}
            }
          }
        }
        rethrow;
      }
    } catch (e) {
      print('[AiApiService v26] JSON íŒŒì‹± ì‹¤íŒ¨: $e');
      print('[AiApiService v26] ì›ë³¸ content (ì²˜ìŒ 200ì): ${content.substring(0, content.length > 200 ? 200 : content.length)}');
      return {'raw': content, '_parse_error': e.toString()};
    }
  }

  /// OpenAI ë¹„ìš© ê³„ì‚°
  ///
  /// ## ê³„ì‚° ê³µì‹
  /// ```
  /// ì…ë ¥ ë¹„ìš© = (promptTokens - cachedTokens) Ã— $2.50/1M
  /// ìºì‹œ ë¹„ìš© = cachedTokens Ã— $1.25/1M (50% í• ì¸)
  /// ì¶œë ¥ ë¹„ìš© = completionTokens Ã— $10.00/1M
  /// ì´ ë¹„ìš© = ì…ë ¥ + ìºì‹œ + ì¶œë ¥
  /// ```
  double _calculateOpenAICost({
    required String model,
    required int promptTokens,
    required int completionTokens,
    int cachedTokens = 0,
  }) {
    final pricing = OpenAIPricing.getModelPricing(model);
    if (pricing == null) return 0.0;

    final inputCost =
        (promptTokens - cachedTokens) * pricing['input']! / 1000000;
    final cachedCost = cachedTokens * pricing['cached']! / 1000000;
    final outputCost = completionTokens * pricing['output']! / 1000000;

    return inputCost + cachedCost + outputCost;
  }

  /// Gemini ë¹„ìš© ê³„ì‚°
  ///
  /// ## ê³„ì‚° ê³µì‹
  /// ```
  /// ì…ë ¥ ë¹„ìš© = promptTokens Ã— $0.10/1M
  /// ì¶œë ¥ ë¹„ìš© = completionTokens Ã— $0.40/1M
  /// ì´ ë¹„ìš© = ì…ë ¥ + ì¶œë ¥
  /// ```
  ///
  /// ## ì°¸ê³ 
  /// GeminiëŠ” í˜„ì¬ ìºì‹œ í• ì¸ ë¯¸ì ìš©
  double _calculateGeminiCost({
    required String model,
    required int promptTokens,
    required int completionTokens,
  }) {
    final pricing = GeminiPricing.getModelPricing(model);
    if (pricing == null) return 0.0;

    final inputCost = promptTokens * pricing['input']! / 1000000;
    final outputCost = completionTokens * pricing['output']! / 1000000;

    return inputCost + outputCost;
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // Chat í¸ì˜ ë©”ì„œë“œ (Fortune ì„œë¹„ìŠ¤ìš©)
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  /// Chat í¸ì˜ ë©”ì„œë“œ - Fortune ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©
  ///
  /// [systemPrompt]ì™€ [userPrompt]ë¥¼ ë°›ì•„ messages ë°°ì—´ë¡œ ë³€í™˜ í›„ API í˜¸ì¶œ
  /// v7.0: Gemini ëª¨ë¸ ìë™ ê°ì§€ ë° ë¼ìš°íŒ…
  /// - gemini-* ëª¨ë¸ â†’ callGemini() ì‚¬ìš©
  /// - ê·¸ ì™¸ â†’ callOpenAI() ì‚¬ìš©
  /// v29: taskType íŒŒë¼ë¯¸í„° ì¶”ê°€ - ë³‘ë ¬ ì‹¤í–‰ ì‹œ task ë¶„ë¦¬ìš©
  Future<ChatResponse> chat({
    required String model,
    required String systemPrompt,
    required String userPrompt,
    int maxTokens = 2000,
    double temperature = 0.7,
    String logType = 'fortune',
    String? userId,
    String taskType = 'saju_analysis',  // v29: ë³‘ë ¬ ì‹¤í–‰ ì‹œ task ë¶„ë¦¬!
  }) async {
    final messages = [
      {'role': 'system', 'content': systemPrompt},
      {'role': 'user', 'content': userPrompt},
    ];

    // v7.0: Gemini ëª¨ë¸ ìë™ ë¼ìš°íŒ…
    final isGemini = model.toLowerCase().contains('gemini');

    if (isGemini) {
      print('[AiApiService] ğŸ”€ Gemini ëª¨ë¸ ê°ì§€ â†’ callGemini() ë¼ìš°íŒ…: $model');
      final response = await callGemini(
        messages: messages,
        model: model,
        maxTokens: maxTokens,
        temperature: temperature,
        logType: logType,
      );

      return ChatResponse(
        success: response.success,
        content: response.content != null
            ? jsonEncode(response.content)
            : null,
        errorMessage: response.error,
        promptTokens: response.promptTokens,
        completionTokens: response.completionTokens,
      );
    }

    // OpenAI ëª¨ë¸ - v29: taskType ì „ë‹¬
    final response = await callOpenAI(
      messages: messages,
      model: model,
      maxTokens: maxTokens,
      temperature: temperature,
      logType: logType,
      userId: userId,
      taskType: taskType,  // v29: ë³‘ë ¬ ì‹¤í–‰ ì‹œ task ë¶„ë¦¬!
    );

    return ChatResponse(
      success: response.success,
      content: response.content != null
          ? jsonEncode(response.content)
          : null,
      errorMessage: response.error,
      promptTokens: response.promptTokens,
      completionTokens: response.completionTokens,
    );
  }
}

/// Chat ì‘ë‹µ í´ë˜ìŠ¤ (Fortune ì„œë¹„ìŠ¤ í˜¸í™˜ìš©)
class ChatResponse {
  final bool success;
  final String? content;
  final String? errorMessage;
  final int? promptTokens;
  final int? completionTokens;

  const ChatResponse({
    required this.success,
    this.content,
    this.errorMessage,
    this.promptTokens,
    this.completionTokens,
  });
}

/// Fortune ì„œë¹„ìŠ¤ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ íƒ€ì… ë³„ì¹­
/// (AIApiService ë„¤ì´ë° ì‚¬ìš© ì¤‘ì¸ ì½”ë“œì™€ í˜¸í™˜)
typedef AIApiService = AiApiService;
